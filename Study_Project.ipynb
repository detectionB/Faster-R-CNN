{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1mwyQXS3eaR-lPyM3sTAUOR1mbCCnNzs0","authorship_tag":"ABX9TyMYAuA4v8fu3sux3dBTg9n9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["0. 데이터셋 압축 해제"],"metadata":{"id":"rQ5SDLCdwEBm"}},{"cell_type":"code","execution_count":39,"metadata":{"id":"MBVY_YhcvkDC","executionInfo":{"status":"ok","timestamp":1767416584624,"user_tz":-540,"elapsed":24,"user":{"displayName":"최현우","userId":"11286671940127226291"}}},"outputs":[],"source":["import zipfile\n","import os\n","\n","# 압축 파일 경로\n","zip_source = '/content/drive/MyDrive/Study/Sample.zip'\n","extract_target = '/content/drive/MyDrive/Study/xray_data'\n","\n","if not os.path.exists(extract_target):\n","    with zipfile.ZipFile(zip_source, 'r') as zip_ref:\n","        zip_ref.extractall(extract_target)\n","    print(\"압축 해제 완료!\")"]},{"cell_type":"markdown","source":["1. 필수 라이브러리 및 파라미터 설정"],"metadata":{"id":"O4fiFwLKwLAt"}},{"cell_type":"code","source":["import os\n","import json\n","import glob\n","import numpy as np\n","import pandas as pd\n","import cv2\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.models import vgg16\n","from torchvision.ops import RoIPool, nms\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from pycocotools.coco import COCO\n","from tqdm import tqdm\n","from collections import namedtuple\n","import six\n","\n","# torchnet 설치 및 임포트\n","!pip install torchnet\n","from torchnet.meter import ConfusionMeter, AverageValueMeter\n","\n","# --- 하이퍼 파라미터 ---\n","epochs = 5\n","learning_rate = 1e-3\n","lr_decay = 0.1\n","weight_decay = 0.0005\n","use_drop = False\n","\n","rpn_sigma = 3.\n","roi_sigma = 1.\n","\n","data_dir = '/content/drive/MyDrive/Study/xray_data/Sample'\n","num_classes = 317  # 배경 제외 클래스 수\n","train_load_path = None\n","inf_load_path = './checkpoints/faster_rcnn_scratch_checkpoints.pth'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UqU7bGOKwHoU","executionInfo":{"status":"ok","timestamp":1767416588890,"user_tz":-540,"elapsed":4261,"user":{"displayName":"최현우","userId":"11286671940127226291"}},"outputId":"6168c61d-def6-4a1d-f835-bb9b8c64d0e9"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchnet in /usr/local/lib/python3.12/dist-packages (0.0.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from torchnet) (2.9.0+cu126)\n","Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from torchnet) (1.17.0)\n","Requirement already satisfied: visdom in /usr/local/lib/python3.12/dist-packages (from torchnet) (0.2.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchnet) (3.5.0)\n","Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.12/dist-packages (from visdom->torchnet) (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from visdom->torchnet) (1.16.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from visdom->torchnet) (2.32.4)\n","Requirement already satisfied: tornado in /usr/local/lib/python3.12/dist-packages (from visdom->torchnet) (6.5.1)\n","Requirement already satisfied: jsonpatch in /usr/local/lib/python3.12/dist-packages (from visdom->torchnet) (1.33)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.12/dist-packages (from visdom->torchnet) (1.9.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from visdom->torchnet) (11.3.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->torchnet) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->torchnet) (3.0.3)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch->visdom->torchnet) (3.0.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->visdom->torchnet) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->visdom->torchnet) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->visdom->torchnet) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->visdom->torchnet) (2025.11.12)\n"]}]},{"cell_type":"markdown","source":["2. 유틸리티 함수"],"metadata":{"id":"SlAFxJkawJVE"}},{"cell_type":"code","source":["def loc2bbox(src_bbox, loc):\n","    if src_bbox.shape[0] == 0:\n","        return np.zeros((0, 4), dtype=loc.dtype)\n","    src_bbox = src_bbox.astype(src_bbox.dtype, copy=False)\n","\n","    src_height = src_bbox[:, 2] - src_bbox[:, 0]\n","    src_width = src_bbox[:, 3] - src_bbox[:, 1]\n","    src_ctr_y = src_bbox[:, 0] + 0.5 * src_height\n","    src_ctr_x = src_bbox[:, 1] + 0.5 * src_width\n","\n","    dy, dx, dh, dw = loc[:, 0::4], loc[:, 1::4], loc[:, 2::4], loc[:, 3::4]\n","\n","    ctr_y = dy * src_height[:, np.newaxis] + src_ctr_y[:, np.newaxis]\n","    ctr_x = dx * src_width[:, np.newaxis] + src_ctr_x[:, np.newaxis]\n","    h = np.exp(dh) * src_height[:, np.newaxis]\n","    w = np.exp(dw) * src_width[:, np.newaxis]\n","\n","    dst_bbox = np.zeros(loc.shape, dtype=loc.dtype)\n","    dst_bbox[:, 0::4], dst_bbox[:, 1::4] = ctr_y - 0.5 * h, ctr_x - 0.5 * w\n","    dst_bbox[:, 2::4], dst_bbox[:, 3::4] = ctr_y + 0.5 * h, ctr_x + 0.5 * w\n","    return dst_bbox\n","\n","def bbox2loc(src_bbox, dst_bbox):\n","    height = src_bbox[:, 2] - src_bbox[:, 0]\n","    width = src_bbox[:, 3] - src_bbox[:, 1]\n","    ctr_y = src_bbox[:, 0] + 0.5 * height\n","    ctr_x = src_bbox[:, 1] + 0.5 * width\n","\n","    base_height = dst_bbox[:, 2] - dst_bbox[:, 0]\n","    base_width = dst_bbox[:, 3] - dst_bbox[:, 1]\n","    base_ctr_y = dst_bbox[:, 0] + 0.5 * base_height\n","    base_ctr_x = dst_bbox[:, 1] + 0.5 * base_width\n","\n","    eps = np.finfo(height.dtype).eps\n","    height, width = np.maximum(height, eps), np.maximum(width, eps)\n","\n","    dy, dx = (base_ctr_y - ctr_y) / height, (base_ctr_x - ctr_x) / width\n","    dh, dw = np.log(base_height / height), np.log(base_width / width)\n","    return np.vstack((dy, dx, dh, dw)).transpose()\n","\n","def normal_init(m, mean, stddev, truncated=False):\n","    if truncated:\n","        m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean)\n","    else:\n","        m.weight.data.normal_(mean, stddev)\n","        if m.bias is not None: m.bias.data.zero_()\n","\n","def get_inside_index(anchor, H, W):\n","    return np.where((anchor[:, 0] >= 0) & (anchor[:, 1] >= 0) & (anchor[:, 2] <= H) & (anchor[:, 3] <= W))[0]\n","\n","def unmap(data, count, index, fill=0):\n","    ret_shape = (count,) if len(data.shape) == 1 else (count,) + data.shape[1:]\n","    ret = np.empty(ret_shape, dtype=data.dtype)\n","    ret.fill(fill)\n","    ret[index] = data\n","    return ret\n","\n","def tonumpy(data):\n","    if isinstance(data, torch.Tensor): return data.detach().cpu().numpy()\n","    return data\n","\n","def totensor(data, cuda=True):\n","    if isinstance(data, np.ndarray): tensor = torch.from_numpy(data)\n","    else: tensor = data.detach()\n","    return tensor.cuda() if cuda else tensor\n","\n","def scalar(data):\n","    return data.item() if isinstance(data, torch.Tensor) else data.reshape(1)[0]\n","\n","def bbox_iou(bbox_a, bbox_b):\n","    tl = np.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n","    br = np.minimum(bbox_a[:, None, 2:], bbox_b[:, 2:])\n","    area_i = np.prod(br - tl, axis=2) * (tl < br).all(axis=2)\n","    area_a = np.prod(bbox_a[:, 2:] - bbox_a[:, :2], axis=1)\n","    area_b = np.prod(bbox_b[:, 2:] - bbox_b[:, :2], axis=1)\n","    return area_i / (area_a[:, None] + area_b - area_i)"],"metadata":{"id":"Ps9w8gQNwQVU","executionInfo":{"status":"ok","timestamp":1767416588960,"user_tz":-540,"elapsed":64,"user":{"displayName":"최현우","userId":"11286671940127226291"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["3. 커스텀 데이터셋"],"metadata":{"id":"aBDSzttBwRzt"}},{"cell_type":"code","source":["class TrainCustom(Dataset):\n","    def __init__(self, data_dir, transforms=True):\n","        self.data_dir = data_dir\n","        self.transforms = transforms\n","\n","        # 모든 이미지 재귀적으로 수집\n","        self.img_paths = sorted(\n","            glob.glob(\n","                os.path.join(data_dir, '01.원천데이터', '**', '*.*'),\n","                recursive=True\n","            )\n","        )\n","        self.img_paths = [\n","            p for p in self.img_paths\n","            if p.lower().endswith(('.jpg', '.jpeg', '.png'))\n","        ]\n","\n","    def img_to_label_path(self, img_path):\n","        \"\"\"\n","        01.원천데이터/Gray(or Color)/정보저장매체/.../xxx.png\n","        → 02.라벨링데이터/정보저장매체/.../xxx.json\n","        \"\"\"\n","        rel = img_path.split('01.원천데이터/')[1]\n","\n","        # Gray / Color 제거\n","        if rel.startswith('Gray/'):\n","            rel = rel.replace('Gray/', '', 1)\n","        elif rel.startswith('Color/'):\n","            rel = rel.replace('Color/', '', 1)\n","\n","        label_path = os.path.join(\n","            self.data_dir,\n","            '02.라벨링데이터',\n","            rel\n","        )\n","\n","        return label_path.replace('.png', '.json').replace('.PNG', '.json') \\\n","                         .replace('.jpg', '.json').replace('.JPG', '.json')\n","\n","    def __getitem__(self, index):\n","        img_path = self.img_paths[index]\n","\n","        # ---- 이미지 로드 ----\n","        image = cv2.imread(img_path)\n","        if image is None:\n","            raise RuntimeError(f\"Failed to read image: {img_path}\")\n","\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n","\n","        # ---- 라벨 로드 ----\n","        label_path = self.img_to_label_path(img_path)\n","        if not os.path.exists(label_path):\n","            raise FileNotFoundError(f\"Label not found:\\n{label_path}\\nFrom image:\\n{img_path}\")\n","\n","        with open(label_path, 'r', encoding='utf-8') as f:\n","            anno = json.load(f)\n","\n","        boxes, labels = [], []\n","        for obj in anno.get('annotations', []):\n","            x, y, w, h = obj['bbox']\n","            boxes.append([x, y, x + w, y + h])\n","            labels.append(obj['category_id'] - 1)\n","\n","        boxes = np.array(boxes, dtype=np.float32)\n","        labels = np.array(labels, dtype=np.int64)\n","\n","        # ---- bbox 없는 이미지 처리 ----\n","        if len(boxes) == 0:\n","            # 최소한 한 개 dummy box (trainer 안정화용)\n","            boxes = np.zeros((1, 4), dtype=np.float32)\n","            labels = np.zeros((1,), dtype=np.int64)\n","\n","        # ---- Transform ----\n","        H, W, _ = image.shape\n","        aug = get_train_transform(H, W) if self.transforms else no_transform()\n","        sample = aug(image=image, bboxes=boxes, labels=labels)\n","\n","        image = sample['image']\n","        bboxes = np.array(sample['bboxes'], dtype=np.float32)\n","        labels = torch.as_tensor(sample[\"labels\"], dtype=torch.int64)\n","\n","        # (x1,y1,x2,y2) → (y1,x1,y2,x2)\n","        if len(bboxes) > 0:\n","            bboxes = bboxes[:, [1, 0, 3, 2]]\n","\n","        return image, torch.tensor(bboxes, dtype=torch.float32), labels, 1.0\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","# Train dataset transform\n","def get_train_transform(h, w):\n","    return A.Compose([\n","        A.Resize(height = h, width = w),\n","        A.HorizontalFlip(p=0.5),\n","        ToTensorV2(p=1.0)\n","    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n","\n","# No transform\n","def no_transform():\n","    return A.Compose([\n","        ToTensorV2(p=1.0) # format for pytorch tensor\n","    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n"],"metadata":{"id":"YYRCOmlCwTFO","executionInfo":{"status":"ok","timestamp":1767416589017,"user_tz":-540,"elapsed":36,"user":{"displayName":"최현우","userId":"11286671940127226291"}}},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":["4. Faster R-CNN 모델 구성 (RPN, Head, Main)"],"metadata":{"id":"ctcJKuYDwT1g"}},{"cell_type":"code","source":["def generate_anchor_base(base_size=16, ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32]):\n","    py, px = base_size / 2., base_size / 2.\n","    anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4), dtype=np.float32)\n","    for i in range(len(ratios)):\n","        for j in range(len(anchor_scales)):\n","            h = base_size * anchor_scales[j] * np.sqrt(ratios[i])\n","            w = base_size * anchor_scales[j] * np.sqrt(1. / ratios[i])\n","            index = i * len(anchor_scales) + j\n","            anchor_base[index, 0], anchor_base[index, 1] = py - h / 2., px - w / 2.\n","            anchor_base[index, 2], anchor_base[index, 3] = py + h / 2., px + w / 2.\n","    return anchor_base\n","\n","class ProposalCreator:\n","    def __init__(self, parent_model, nms_thresh=0.7, n_train_pre_nms=12000, n_train_post_nms=2000, n_test_pre_nms=6000, n_test_post_nms=300, min_size=16):\n","        self.parent_model = parent_model\n","        self.nms_thresh, self.min_size = nms_thresh, min_size\n","        self.n_train_pre_nms, self.n_train_post_nms = n_train_pre_nms, n_train_post_nms\n","        self.n_test_pre_nms, self.n_test_post_nms = n_test_pre_nms, n_test_post_nms\n","\n","    def __call__(self, loc, score, anchor, img_size, scale=1.):\n","        n_pre, n_post = (self.n_train_pre_nms, self.n_train_post_nms) if self.parent_model.training else (self.n_test_pre_nms, self.n_test_post_nms)\n","        roi = loc2bbox(anchor, loc)\n","        roi[:, slice(0, 4, 2)] = np.clip(roi[:, slice(0, 4, 2)], 0, img_size[0])\n","        roi[:, slice(1, 4, 2)] = np.clip(roi[:, slice(1, 4, 2)], 0, img_size[1])\n","\n","        hs, ws = roi[:, 2] - roi[:, 0], roi[:, 3] - roi[:, 1]\n","        keep = np.where((hs >= self.min_size * scale) & (ws >= self.min_size * scale))[0]\n","        roi, score = roi[keep, :], score[keep]\n","\n","        order = score.ravel().argsort()[::-1][:n_pre]\n","        roi, score = roi[order, :], score[order]\n","\n","        keep = nms(torch.from_numpy(roi).cuda(), torch.from_numpy(score).cuda(), self.nms_thresh)\n","        return roi[keep[:n_post].cpu().numpy()]\n","\n","class RegionProposalNetwork(nn.Module):\n","    def __init__(self, in_channels=512, mid_channels=512, ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32], feat_stride=16):\n","        super().__init__()\n","        self.anchor_base = generate_anchor_base(anchor_scales=anchor_scales, ratios=ratios)\n","        self.feat_stride = feat_stride\n","        self.proposal_layer = ProposalCreator(self)\n","        n_anchor = self.anchor_base.shape[0]\n","        self.conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n","        self.score = nn.Conv2d(mid_channels, n_anchor * 2, 1, 1, 0)\n","        self.loc = nn.Conv2d(mid_channels, n_anchor * 4, 1, 1, 0)\n","        for m in [self.conv1, self.score, self.loc]: normal_init(m, 0, 0.01)\n","\n","    def forward(self, x, img_size, scale=1.):\n","        n, _, hh, ww = x.shape\n","        anchor = _enumerate_shifted_anchor(self.anchor_base, self.feat_stride, hh, ww)\n","        n_anchor = anchor.shape[0] // (hh * ww)\n","        middle = F.relu(self.conv1(x))\n","        rpn_locs = self.loc(middle).permute(0, 2, 3, 1).contiguous().view(n, -1, 4)\n","        rpn_scores = self.score(middle).permute(0, 2, 3, 1).contiguous()\n","        rpn_fg_scores = F.softmax(rpn_scores.view(n, hh, ww, n_anchor, 2), dim=4)[:, :, :, :, 1].contiguous().view(n, -1)\n","        rpn_scores = rpn_scores.view(n, -1, 2)\n","\n","        rois, roi_indices = [], []\n","        for i in range(n):\n","            roi = self.proposal_layer(rpn_locs[i].cpu().data.numpy(), rpn_fg_scores[i].cpu().data.numpy(), anchor, img_size, scale)\n","            rois.append(roi)\n","            roi_indices.append(i * np.ones((len(roi),), dtype=np.int32))\n","        return rpn_locs, rpn_scores, np.concatenate(rois), np.concatenate(roi_indices), anchor\n","\n","def _enumerate_shifted_anchor(anchor_base, feat_stride, height, width):\n","    shift_y, shift_x = np.arange(0, height * feat_stride, feat_stride), np.arange(0, width * feat_stride, feat_stride)\n","    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n","    shift = np.stack((shift_y.ravel(), shift_x.ravel(), shift_y.ravel(), shift_x.ravel()), axis=1)\n","    K, A = shift.shape[0], anchor_base.shape[0]\n","    return (anchor_base.reshape((1, A, 4)) + shift.reshape((1, K, 4)).transpose((1, 0, 2))).reshape((K * A, 4)).astype(np.float32)\n","\n","def decom_vgg16():\n","    model = vgg16(pretrained=True)\n","    features = list(model.features)[:30]\n","    classifier = list(model.classifier)\n","    del classifier[6]\n","    if not use_drop: del classifier[5], classifier[2]\n","    for layer in features[:10]:\n","        for p in layer.parameters(): p.requires_grad = False\n","    return nn.Sequential(*features), nn.Sequential(*classifier)\n","\n","class VGG16RoIHead(nn.Module):\n","    def __init__(self, n_class, roi_size, spatial_scale, classifier):\n","        super().__init__()\n","        self.classifier = classifier\n","        self.cls_loc, self.score = nn.Linear(4096, n_class * 4), nn.Linear(4096, n_class)\n","        normal_init(self.cls_loc, 0, 0.001); normal_init(self.score, 0, 0.01)\n","        self.n_class, self.roi_size, self.spatial_scale = n_class, roi_size, spatial_scale\n","        self.roi = RoIPool((roi_size, roi_size), spatial_scale)\n","\n","    def forward(self, x, rois, roi_indices):\n","        roi_indices, rois = totensor(roi_indices).float(), totensor(rois).float()\n","        indices_and_rois = torch.cat([roi_indices[:, None], rois], dim=1)\n","        xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]].contiguous()\n","        pool = self.roi(x, xy_indices_and_rois).view(indices_and_rois.size(0), -1)\n","        fc7 = self.classifier(pool)\n","        return self.cls_loc(fc7), self.score(fc7)\n","\n","class FasterRCNN(nn.Module):\n","    def __init__(self, extractor, rpn, head):\n","        super().__init__()\n","        self.extractor, self.rpn, self.head = extractor, rpn, head\n","        self.loc_normalize_mean, self.loc_normalize_std = (0., 0., 0., 0.), (0.1, 0.1, 0.2, 0.2)\n","        self.nms_thresh, self.score_thresh = 0.3, 0.05\n","\n","    def forward(self, x, scale=1.):\n","        img_size = x.shape[2:]\n","        h = self.extractor(x)\n","        rpn_locs, rpn_scores, rois, roi_indices, _ = self.rpn(h, img_size, scale)\n","        roi_cls_locs, roi_scores = self.head(h, rois, roi_indices)\n","        return roi_cls_locs, roi_scores, rois, roi_indices\n","\n","    @torch.no_grad()\n","    def predict(self, imgs, sizes):\n","        self.eval()\n","        bboxes, labels, scores = [], [], []\n","        for img, size in zip(imgs, sizes):\n","            img = totensor(img[None]).float()\n","            scale = img.shape[3] / size[1]\n","            roi_cls_loc, roi_score, rois, _ = self(img, scale=scale)\n","            roi = totensor(rois) / scale\n","\n","            mean = torch.Tensor(self.loc_normalize_mean).cuda().repeat(self.head.n_class)[None]\n","            std = torch.Tensor(self.loc_normalize_std).cuda().repeat(self.head.n_class)[None]\n","            roi_cls_loc = (roi_cls_loc * std + mean).view(-1, self.head.n_class, 4)\n","            roi = roi.view(-1, 1, 4).expand_as(roi_cls_loc)\n","\n","            cls_bbox = loc2bbox(tonumpy(roi).reshape(-1, 4), tonumpy(roi_cls_loc).reshape(-1, 4))\n","            cls_bbox = totensor(cls_bbox).view(-1, self.head.n_class * 4)\n","            cls_bbox[:, 0::2].clamp_(0, size[0]); cls_bbox[:, 1::2].clamp_(0, size[1])\n","\n","            prob = F.softmax(roi_score, dim=1)\n","            bbox_p, label_p, score_p = self._suppress(cls_bbox, prob)\n","            bboxes.append(bbox_p); labels.append(label_p); scores.append(score_p)\n","        self.train()\n","        return bboxes, labels, scores\n","\n","    def _suppress(self, raw_cls_bbox, raw_prob):\n","        bbox, label, score = [], [], []\n","        for l in range(1, self.head.n_class):\n","            cls_bbox_l = raw_cls_bbox.view(-1, self.head.n_class, 4)[:, l, :]\n","            prob_l = raw_prob[:, l]\n","            mask = prob_l > self.score_thresh\n","            cls_bbox_l, prob_l = cls_bbox_l[mask], prob_l[mask]\n","            keep = nms(cls_bbox_l, prob_l, self.nms_thresh)\n","            bbox.append(cls_bbox_l[keep].cpu().numpy())\n","            label.append((l - 1) * np.ones((len(keep),)))\n","            score.append(prob_l[keep].cpu().numpy())\n","        return np.concatenate(bbox), np.concatenate(label), np.concatenate(score)\n","\n","    def get_optimizer(self):\n","        params = []\n","        for key, value in self.named_parameters():\n","            if value.requires_grad:\n","                lr = learning_rate * (2 if 'bias' in key else 1)\n","                wd = 0 if 'bias' in key else weight_decay\n","                params.append({'params': [value], 'lr': lr, 'weight_decay': wd})\n","        self.optimizer = torch.optim.SGD(params, momentum=0.9)\n","        return self.optimizer\n","\n","class FasterRCNNVGG16(FasterRCNN):\n","    def __init__(self, n_fg_class=317):\n","        extractor, classifier = decom_vgg16()\n","        rpn = RegionProposalNetwork(512, 512)\n","        head = VGG16RoIHead(n_class=n_fg_class + 1, roi_size=7, spatial_scale=1/16, classifier=classifier)\n","        super().__init__(extractor, rpn, head)"],"metadata":{"id":"fv7UhLiswXrV","executionInfo":{"status":"ok","timestamp":1767416589041,"user_tz":-540,"elapsed":18,"user":{"displayName":"최현우","userId":"11286671940127226291"}}},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":["5. Trainer 클래스"],"metadata":{"id":"dh0ezw2uwZZl"}},{"cell_type":"code","source":["class AnchorTargetCreator:\n","    def __init__(self, n_sample=256, pos_iou_thresh=0.7, neg_iou_thresh=0.3, pos_ratio=0.5):\n","        self.n_sample, self.pos_iou_thresh, self.neg_iou_thresh, self.pos_ratio = n_sample, pos_iou_thresh, neg_iou_thresh, pos_ratio\n","\n","    def __call__(self, bbox, anchor, img_size):\n","        H, W = img_size\n","        inside_index = get_inside_index(anchor, H, W)\n","        anchor_inside = anchor[inside_index]\n","\n","        argmax_ious, label = self._create_label(inside_index, anchor_inside, bbox)\n","        loc = bbox2loc(anchor_inside, bbox[argmax_ious])\n","\n","        return unmap(loc, len(anchor), inside_index, 0), unmap(label, len(anchor), inside_index, -1)\n","\n","    def _create_label(self, inside_index, anchor, bbox):\n","        label = -np.ones((len(inside_index),), dtype=np.int32)\n","        ious = bbox_iou(anchor, bbox)\n","        argmax_ious = ious.argmax(axis=1)\n","        max_ious = ious.max(axis=1)\n","        gt_argmax_ious = ious.argmax(axis=0)\n","\n","        label[max_ious < self.neg_iou_thresh] = 0\n","        label[gt_argmax_ious] = 1\n","        label[max_ious >= self.pos_iou_thresh] = 1\n","\n","        pos_index = np.where(label == 1)[0]\n","        n_pos = int(self.pos_ratio * self.n_sample)\n","        if len(pos_index) > n_pos:\n","            label[np.random.choice(pos_index, len(pos_index) - n_pos, replace=False)] = -1\n","\n","        neg_index = np.where(label == 0)[0]\n","        n_neg = self.n_sample - np.sum(label == 1)\n","        if len(neg_index) > n_neg:\n","            label[np.random.choice(neg_index, len(neg_index) - n_neg, replace=False)] = -1\n","        return argmax_ious, label\n","\n","class ProposalTargetCreator:\n","    def __init__(self, n_sample=128, pos_ratio=0.25, pos_iou_thresh=0.5, neg_iou_thresh_hi=0.5, neg_iou_thresh_lo=0.0):\n","        self.n_sample, self.pos_ratio, self.pos_iou_thresh = n_sample, pos_ratio, pos_iou_thresh\n","        self.neg_iou_thresh_hi, self.neg_iou_thresh_lo = neg_iou_thresh_hi, neg_iou_thresh_lo\n","\n","    def __call__(self, roi, bbox, label, loc_normalize_mean=(0.,0.,0.,0.), loc_normalize_std=(0.1,0.1,0.2,0.2)):\n","        roi = np.concatenate((roi, bbox), axis=0)\n","        ious = bbox_iou(roi, bbox)\n","        gt_assignment = ious.argmax(axis=1)\n","        max_iou = ious.max(axis=1)\n","        gt_roi_label = label[gt_assignment] + 1\n","\n","        pos_index = np.where(max_iou >= self.pos_iou_thresh)[0]\n","        n_pos = int(min(np.round(self.n_sample * self.pos_ratio), pos_index.size))\n","        if pos_index.size > 0: pos_index = np.random.choice(pos_index, size=n_pos, replace=False)\n","\n","        neg_index = np.where((max_iou < self.neg_iou_thresh_hi) & (max_iou >= self.neg_iou_thresh_lo))[0]\n","        n_neg = int(min(self.n_sample - n_pos, neg_index.size))\n","        if neg_index.size > 0: neg_index = np.random.choice(neg_index, size=n_neg, replace=False)\n","\n","        keep_index = np.append(pos_index, neg_index)\n","        sample_roi = roi[keep_index]\n","        gt_roi_label = gt_roi_label[keep_index]\n","        gt_roi_label[n_pos:] = 0\n","        gt_roi_loc = bbox2loc(sample_roi, bbox[gt_assignment[keep_index]])\n","        gt_roi_loc = (gt_roi_loc - np.array(loc_normalize_mean)) / np.array(loc_normalize_std)\n","        return sample_roi, gt_roi_loc, gt_roi_label\n","\n","class FasterRCNNTrainer(nn.Module):\n","    def __init__(self, faster_rcnn):\n","        super().__init__()\n","        self.faster_rcnn = faster_rcnn\n","        self.anchor_target_creator = AnchorTargetCreator()\n","        self.proposal_target_creator = ProposalTargetCreator()\n","        self.optimizer = faster_rcnn.get_optimizer()\n","\n","        # 수정: ConfusionMeter 크기를 실제 클래스 개수(+배경)에 맞게 설정\n","        self.rpn_cm = ConfusionMeter(2)\n","        self.roi_cm = ConfusionMeter(num_classes + 1)\n","        self.meters = {k: AverageValueMeter() for k in ['rpn_loc_loss', 'rpn_cls_loss', 'roi_loc_loss', 'roi_cls_loss', 'total_loss']}\n","\n","    def forward(self, imgs, bboxes, labels, scale):\n","        _, _, H, W = imgs.shape\n","        features = self.faster_rcnn.extractor(imgs)\n","        rpn_locs, rpn_scores, rois, roi_indices, anchor = self.faster_rcnn.rpn(features, (H, W), scale)\n","\n","        bbox, label = bboxes[0], labels[0]\n","        sample_roi, gt_roi_loc, gt_roi_label = self.proposal_target_creator(rois, tonumpy(bbox), tonumpy(label))\n","        roi_cls_loc, roi_score = self.faster_rcnn.head(features, sample_roi, torch.zeros(len(sample_roi)))\n","\n","        # Losses\n","        gt_rpn_loc, gt_rpn_label = self.anchor_target_creator(tonumpy(bbox), anchor, (H, W))\n","        rpn_cls_loss = F.cross_entropy(rpn_scores[0], totensor(gt_rpn_label).long(), ignore_index=-1)\n","        rpn_loc_loss = _fast_rcnn_loc_loss(rpn_locs[0], totensor(gt_rpn_loc), totensor(gt_rpn_label).long(), rpn_sigma)\n","\n","        roi_cls_loss = F.cross_entropy(roi_score, totensor(gt_roi_label).long())\n","        n_sample = roi_cls_loc.shape[0]\n","        roi_loc = roi_cls_loc.view(n_sample, -1, 4)[torch.arange(n_sample).long().cuda(), totensor(gt_roi_label).long()]\n","        roi_loc_loss = _fast_rcnn_loc_loss(roi_loc, totensor(gt_roi_loc), totensor(gt_roi_label).long(), roi_sigma)\n","\n","        losses = [rpn_loc_loss, rpn_cls_loss, roi_loc_loss, roi_cls_loss]\n","        total_loss = sum(losses)\n","\n","        # Metrics update\n","        self.rpn_cm.add(rpn_scores[0][gt_rpn_label > -1].detach(), totensor(gt_rpn_label[gt_rpn_label > -1]).long())\n","        self.roi_cm.add(roi_score.detach(), totensor(gt_roi_label).long())\n","\n","        loss_dict = {'rpn_loc_loss': rpn_loc_loss, 'rpn_cls_loss': rpn_cls_loss, 'roi_loc_loss': roi_loc_loss, 'roi_cls_loss': roi_cls_loss, 'total_loss': total_loss}\n","        for k, v in loss_dict.items(): self.meters[k].add(scalar(v))\n","\n","        return total_loss\n","\n","    def train_step(self, imgs, bboxes, labels, scale):\n","        self.optimizer.zero_grad()\n","        loss = self.forward(imgs, bboxes, labels, scale)\n","        loss.backward()\n","        self.optimizer.step()\n","        return loss\n","\n","    def save(self, path='./checkpoints/faster_rcnn_scratch.pth'):\n","        os.makedirs(os.path.dirname(path), exist_ok=True)\n","        torch.save({'model': self.faster_rcnn.state_dict(), 'optimizer': self.optimizer.state_dict()}, path)\n","\n","def _smooth_l1_loss(x, t, in_weight, sigma):\n","    sigma2 = sigma ** 2\n","    diff = in_weight * (x - t)\n","    abs_diff = diff.abs()\n","    flag = (abs_diff.data < (1. / sigma2)).float()\n","    return (flag * (sigma2 / 2.) * (diff ** 2) + (1 - flag) * (abs_diff - 0.5 / sigma2)).sum()\n","\n","def _fast_rcnn_loc_loss(pred_loc, gt_loc, gt_label, sigma):\n","    in_weight = torch.zeros(gt_loc.shape).cuda()\n","    in_weight[(gt_label > 0).view(-1, 1).expand_as(in_weight)] = 1\n","    loc_loss = _smooth_l1_loss(pred_loc, gt_loc, in_weight.detach(), sigma)\n","    return loc_loss / ((gt_label >= 0).sum().float())"],"metadata":{"id":"WJcAEMQqwblk","executionInfo":{"status":"ok","timestamp":1767416589045,"user_tz":-540,"elapsed":17,"user":{"displayName":"최현우","userId":"11286671940127226291"}}},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":["6. 실행 (Train & Inference)"],"metadata":{"id":"kOrsB1Q5ww9d"}},{"cell_type":"code","source":["def train():\n","    dataset = TrainCustom(data_dir)\n","    dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n","    model = FasterRCNNVGG16(num_classes).cuda()\n","    trainer = FasterRCNNTrainer(model).cuda()\n","\n","    best_loss = 1e9\n","    for epoch in range(epochs):\n","        trainer.meters = {k: AverageValueMeter() for k in trainer.meters.keys()}\n","        for img, bbox, label, scale in tqdm(dataloader):\n","            trainer.train_step(img.cuda(), bbox.cuda(), label.cuda(), float(scale))\n","\n","        avg_loss = trainer.meters['total_loss'].value()[0]\n","        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n","        if avg_loss < best_loss:\n","            best_loss = avg_loss\n","            trainer.save()\n","\n","def inference():\n","    testset = TestCustom(data_dir)\n","    test_loader = DataLoader(testset, batch_size=1, shuffle=False)\n","    model = FasterRCNNVGG16(num_classes).cuda()\n","    model.load_state_dict(torch.load(inf_load_path)['model'])\n","\n","    coco = COCO(os.path.join(data_dir, 'test.json'))\n","    results = []\n","\n","    for i, (img, size) in enumerate(tqdm(test_loader)):\n","        h, w = size[0].item(), size[1].item()\n","        bboxes, labels, scores = model.predict([img[0]], [(h, w)])\n","\n","        prediction_string = \"\"\n","        for b, l, s in zip(bboxes[0], labels[0], scores[0]):\n","            # 순서: label score xmin ymin xmax ymax\n","            prediction_string += f\"{int(l)} {s:.4f} {b[1]} {b[0]} {b[3]} {b[2]} \"\n","\n","        img_id = coco.getImgIds()[i]\n","        results.append({'PredictionString': prediction_string, 'image_id': coco.loadImgs(img_id)[0]['file_name']})\n","\n","    pd.DataFrame(results).to_csv(\"./submission.csv\", index=False)"],"metadata":{"id":"MICf0GBAw1Ec","executionInfo":{"status":"ok","timestamp":1767416589084,"user_tz":-540,"elapsed":30,"user":{"displayName":"최현우","userId":"11286671940127226291"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["dataset = TrainCustom(data_dir)\n","print(\"Number of images:\", len(dataset))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O8xNEAzU04P7","executionInfo":{"status":"ok","timestamp":1767416589478,"user_tz":-540,"elapsed":390,"user":{"displayName":"최현우","userId":"11286671940127226291"}},"outputId":"1d9c4983-26a5-46de-f6dd-0d14b6e58fba"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of images: 5002\n"]}]},{"cell_type":"code","source":["img, bboxes, labels, scale = dataset[0]\n","\n","print(\"img:\", img.shape)\n","print(\"bboxes:\", bboxes.shape)\n","print(\"labels:\", labels.shape)\n","print(\"labels unique:\", labels.unique())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wrTZg4Gq26po","executionInfo":{"status":"ok","timestamp":1767416589537,"user_tz":-540,"elapsed":40,"user":{"displayName":"최현우","userId":"11286671940127226291"}},"outputId":"99f90d55-f329-4b8b-8688-f89cf1229685"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["img: torch.Size([3, 760, 896])\n","bboxes: torch.Size([1, 4])\n","labels: torch.Size([1])\n","labels unique: tensor([70])\n"]}]},{"cell_type":"code","source":["train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XOa5c34_OA7r","outputId":"257b5047-5200-4b6b-9426-9c452ca21495"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"," 34%|███▍      | 1700/5002 [13:22<30:38,  1.80it/s]"]}]},{"cell_type":"code","source":[],"metadata":{"id":"lCkAuSFcODPh"},"execution_count":null,"outputs":[]}]}